{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d0d20974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax Regression Starts!\n",
      "Loading Time: 4.706471920013428s.\n",
      "Training Time:  37.34835600852966s\n",
      "Testing Time: 0.11655998229980469s\n",
      "Accuraccy: 0.9730337078651685\n",
      "Softmax Regression Ends!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import csv\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class SoftmaxRegression():\n",
    "    def __init__(self, class_list):\n",
    "        self.weight = np.array([], dtype=float) # k * n + 1 \n",
    "        self.k = len(class_list)\n",
    "        self.index_to_label = {}\n",
    "        self.label_to_index = {}\n",
    "        for i, label in enumerate(class_list):\n",
    "            self.index_to_label[i] = label\n",
    "            self.label_to_index[label] = i\n",
    "\n",
    "    def softmax(self, X):\n",
    "        matrix = np.exp(np.dot(self.weight, X.T)) #softmax activation function for the input data matrix X\n",
    "        return matrix / np.sum(matrix, axis=0)\n",
    "\n",
    "    #make predictions using the trained softmax regression model.\n",
    "    def predict(self, X):\n",
    "        X = np.array(X) #converts the input data X into an array which is the desired format for further processing\n",
    "        sample_nums = X.shape[0]\n",
    "        X = np.column_stack((np.ones(sample_nums), X)) # to incorporate the bias term into the input data\n",
    "        index_result = np.argmax(self.softmax(X), axis=0) #determines the index of the class with the highest probability for each sample \n",
    "        return [self.index_to_label[index] for index in index_result]#maps these indices back to the corresponding class labels\n",
    "\n",
    "\n",
    "    # trains the softmax regression model by iteratively updating the weights using batch gradient descent, to minimize the loss function and improve prediction.\n",
    "    def fit_BGD(self, X, y, alpha=0.01, reg=0.1, max_iter=1000, epsilon=1e-10):#gradient descent\n",
    "        X = np.array(X)\n",
    "        sample_nums, feature_nums = X.shape[0], X.shape[1] + 1#bias\n",
    "        Y = np.zeros((self.k, sample_nums))\n",
    "        for i, label in enumerate(y):\n",
    "            Y[self.label_to_index[label], i] = 1\n",
    "        X = np.column_stack((np.ones(sample_nums), X))\n",
    "        self.weight = np.zeros((self.k, feature_nums), dtype=float)\n",
    "        for i in range(max_iter):\n",
    "            batch_gradient = np.dot((Y - self.softmax(X)), X)  / sample_nums\n",
    "            self.weight += (alpha * batch_gradient - reg * self.weight)# Updates the weight matrix using the computed gradient. The weights are adjusted in the direction that minimizes the loss function, scaled by the learning rate (alpha) and the regularization term (reg). This step is the heart of the Batch Gradient Descent algorithm.\n",
    "        return self\n",
    "\n",
    "    def score(self, X, y_true):\n",
    "        return accuracy_score(y_true, self.predict(X))\n",
    "\n",
    "\n",
    "TRAINING_NEWS_FILE = 'bbc_news_train.csv'\n",
    "TEST_FILE = 'bbc_news_test.csv'\n",
    "NON_WORD_CHAR = re.compile(r'\\W+')\n",
    "\n",
    "def get_csv_data(path):\n",
    "    data_list = []\n",
    "    with open(path) as csvfile:\n",
    "        csvreader = csv.reader(csvfile)\n",
    "        for row in csvreader:\n",
    "            data_list.append((tokenize(row[0]), row[1]))\n",
    "    return data_list\n",
    "\n",
    "def tokenize(string): \n",
    "    true_tokens = []\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    possible_tokens = [NON_WORD_CHAR.sub('', word) for word in string.lower().strip().split() if word not in stop_words]\n",
    "    for possible in possible_tokens:\n",
    "        if len(possible) > 0:\n",
    "            true_tokens.append(possible)\n",
    "    return true_tokens\n",
    "\n",
    "def get_feature_words(training_set, size = 1000):\n",
    "    feature_words_dict = {}\n",
    "    for element in training_set:\n",
    "        for word in element[0]:\n",
    "            if word in feature_words_dict:\n",
    "                feature_words_dict[word] += 1\n",
    "            else:\n",
    "                feature_words_dict[word] = 1\n",
    "    feature_words_tuple = sorted(feature_words_dict.items(), key=lambda x:x[1], reverse=True)\n",
    "    feature_words = list(list(zip(*feature_words_tuple))[0])\n",
    "    return feature_words[:size] if len(feature_words) > size else feature_words\n",
    "\n",
    "def train_test_extract(training_set, test_data, feature_words):\n",
    "    X_train = [[1 if word in element[0] else 0 for word in feature_words] for element in training_set]\n",
    "    y_train = [element[1] for element in training_set]\n",
    "    X_test = [[1 if word in element[0] else 0 for word in feature_words] for element in test_data]\n",
    "    y_test = [element[1] for element in test_data]\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "def get_category_list(training_set):\n",
    "    local_category_list = set()\n",
    "    for row in training_set:\n",
    "        local_category_list.add(row[1])\n",
    "    category_list = list(local_category_list)\n",
    "    category_list.sort()\n",
    "    return category_list\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    print(\"Softmax Regression Starts!\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    train_data = get_csv_data(TRAINING_NEWS_FILE)\n",
    "    test_data = get_csv_data(TEST_FILE)\n",
    "    feature_words = get_feature_words(train_data, size=1000)\n",
    "    X_train, y_train, X_test, y_test = train_test_extract(train_data, test_data, feature_words)\n",
    "    category_list = get_category_list(train_data)\n",
    "    print(\"Loading Time: %ss.\" % str(time.time()-start_time))\n",
    "\n",
    "    start_time = time.time()\n",
    "    clf_BGD = SoftmaxRegression(category_list).fit_BGD(X_train, y_train, alpha=0.1, reg=0.01, max_iter=2000, epsilon=0.0)\n",
    "    print(\"Training Time:  %ss\" % (str(time.time()-start_time)))\n",
    "\n",
    "    start_time = time.time()\n",
    "    test_accuracy = clf_BGD.score(X_test, y_test)\n",
    "    print(\"Testing Time: %ss\" % (str(time.time()-start_time)))\n",
    "\n",
    "    print(\"Accuraccy: %s\" % str(test_accuracy))\n",
    "    \n",
    "    print(\"Softmax Regression Ends!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2daad3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41edba6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5282b79f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d29f222",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
